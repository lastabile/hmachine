
Informal doc and notes for hmachine work

To retain c file in compiling:
(let ((COMPILER::*DEFAULT-C-FILE* "h.c")) (compile-file "h.lisp"))

- objects need attached rules
- rules are also objects

- Nodes have special names nddd...d
- Attribute names are atoms
- Vars are nodes ?<x>, where <x> is any char sequence and ?<x> forms a symbol -- technically not needed; a syntactic convenience
- Values can be numbers, strings, or symbols. Values are a shorthand for more graph structure

Most primitive syntax is edges:

(n42 type rule)
(n42 pred n43)
(n43 type rule-and)
(n43 left n44)
(n43 right n45)

Next sugar might be:

(n42 type rule pred n43)

Syntax for a rule:

Vars are bound by the "rule" keyword, and we lexically close any
embedded rules.

The keyword "rule" is similar to the keyword "lambda", but the syntax
and operational semantics of h are sufficiently different from
lambda-calc that we don't want to reuse the term.


One meta-leval syntax:

(rule (x y z)
  (pred (x color red)
		(x size 42))
  (add (x length short)
	   (rule (a b)
		 (pred (a height y))
		 (add (a b x)))))

Another meta-level syntax, which is representable as a higher-order
hypergraph. Need idea of "new node"; however in this and the previous
syntax we might say that any free vars are really new nodes, within
that lexcial scope.

However, can't tell difference between those and existing nodes on
which we match. We could say any non-matching nodes in the the add
clause are new -- matches such as pred, vars, etc. are not

(x var)
(y var)
(z var)
(x color red pred)
(x size 42 pred)
(x length short add)
(x rule n1 add)
(n1 vars n2 add)
(n1 pred n3 add)
(n1 add n4 add)
(n2 elem u add)
(n2 elem v add)


Base-level (in H-graph) rule syntax is labelled graph:

Vars are in scope, with the first rule node considered the outer
scope. This way rules can be generated and use the results of
previously matched rules.

  r -- rule
  r.vars.elem.*
  r.pred.edge.elem.*
  r.add.edge.elem.*
  r.del.edge.elem.*

Eg this graph:

(r vars v1)
(r pred p1)
(r add a1)
(r del d1)
(v1 elem x)
(v1 elem y)
(v1 elem z)
(p1 edge pe1)
(p1 edge pe2)
(pe1 elem x)
(pe1 elem a)
(pe1 elem 42)
(a1 edge ae1)
(a1 edge ae2)
(ae1 elem x)
(ae1 elem length)
(ae1 elem short)
(r new-nodes nn)
(nn elem n1)
(nn elem n2)
(x rule n1)
(n1 (vars u v w) (pred ... v ...))

"Relation Set" notation -- 

(r (var (x y z))) == 

					(r vars v1)
					(v1 elem x)
					(v1 elem y)
					(v1 elem z)

(r (pred (x color red)(x size 42))) ==

(r pred p1)
(r pred p2)
(p1 elem x)
(p1 elem color)
(p1 elem red)
(p2 elem x)
(p2 elem size)
(p2 elem 42)


(rule (vars (x y z))
	  (pred (x color red)
			(x size 42)
			(x next-door y))
	  (add (x length short)
		   (x rule (rule (vars (a b))
						 (pred (y height a)
							   (y size 57))
						 (add (y length long)
							  (b width short))))))

(rule (vars (x y z))
	  (pred (x color red)
			(x size 42)
			(x next-door y))
	  (add (x length short)
		   (y height 57)))

Another possible syntax for a rule (currently not implemented):

(rule (?x color red)
	  (?x size large)
	  (?x color red del)
	  (?x color blue add))

Pred edges are unmarked, and add and del have those prefixes (or suffixes).

del's and add's are not ordered relative to the pred edges, but but
convention the del's and add's are at the end.

----------------------------------------------------------
Formal syntax and semantics write-up
Started 3/9/17

<node>               ::= <var> | <const>
<var>                ::= <var-symbol>
<const>              ::= <const-symbol> | <number> | <string>
<var-symbol>         ::= <symbol-beginning-with-?>
<const-symbol>       ::= <symbol-not-beginning-with-?>
<edge>               ::= (<node> ...)
<rule>               ::= (rule
                          [(name <name>)]
                          [(root-var <var>)]
                          [(attach-to <node>)]
                          [(local)]
                          (pred
                           <pred-edge> ...)
                          (not
                           <not-edge> ...)
                          (add
                           <add-edge> ...)
                          (del
                           <del-edge> ...))
<pred-edge>          ::= (<node> ...) | (<var> new-node <new-node-indicator>)
<not-edge>           ::= (<node> ...)
<add-edge>           ::= (<node> ...) | (print <node> ...) | (<node> rule <rule>)
<del-edge>           ::= (<node> ...)
<new-node-indicator> ::= nn<number>

Note liberties taken with the BNF above: Each disjunction does not
necesarily represent a disjoint partitioning -- read it as
more-general-to-more-specific, left-to-right. So e.g. an <add-edge>
is generally a list of nodes but has a special "print" form and "rule"
form.

[(name <name>)]

Optional name assigned to a rule. Generally essential when matching
against rules for propragation purposes. Although in many cases a rule
can be identified by matching on some of its components, it's normally
better to give it a name so it's easily specified and found.

[(local])

If this is present, then the rule is local, and is thus not in the
global rule pool, and will not be matched in the course of executing a
node. If absent, then, the rule is global.

See below for info on node execution and rule resolution.

[(attach-to <node>)]

If present, the rule is automatically made local and attached to the
given node. Typically this is used with the global-node for data
inits and so forth.

[(root-var <node>)]

<node> here is a node within the rule. It's typically a var, but may
be a const as well.  Normally, without this present, the system will
match a node against a rule by trying all orientations of the rule
nodes against the given object node. This method is general and helps
assure a match even when you're not sure you're attaching the rule to
the "right" node. Good performance depends on being able to detect the
non-matches quickly, and having rules that don't over-match. While
this generally works, if the root var is specified, then it is the
only orientation matched against, which can help performance.

Rule representation and semantics

Rules are represented within a graph g by expanding the edge lists in
pred, add, etc., into nodes that explicitly define the edge content as
relations in g. For example

(r pred 
   (?x next ?y)) =>

(r pred p1)
(p1 elem0 ?x)
(p1 elem1 next)
(p1 elem2 ?y)

Note that the elem<n> symbols become special indicators for rule
processing. One can therefore match directly against rule content, and
rules are parts of the graph. However, as with the rest of the H
language, there are no reserved symbols and these may be used in rules
and data as any other symbol can.

Generating rules

Rules can be generated by other rules using the "elem" edge
representation.  However this is rather tedious and hence some
syntactic sugar is called for. The nested rule construct shown in the
BNF above defines a new rule, and expands it into elem edges. It does
so when its rule matches; hence any bound variables will be
substituted into the generated rule. This is a very useful mechanism,
and it's possible for one rule to match data such that a whole family
of rules is generated. [see rule-30 stuff]

Execution

A program in H is essentially a sequence of rule definitions. Rules
are used both to define literal data, and derive new data from old.

In [C&C] the fundamental computational mechanism is described as an
exhaustive search through all subgraphs of a hypergraph, comparing
each subgraph against another, and deriving new edges when a match
occurs. For a finite graph this is at least computable, but hardly
efficient. 

To make this practical, some reasonable means of control must be
introduced. This is accomplished as follows.

A node is *executed* by running all rules accessible by the node.
By "running" a rule we mean matching the rule against a set of
subgraphs found in a neighborhood around the node, and deriving any
new edges from any set of matches.  The nature of this neighborhood
is an important part of the performance of the H machine and is
described in detail in section [xxx]. Note that more than one matching
subgraph may be found in a neighborhood; each of them can produce new
edges via the add clause of the rule.

Rules are accessible by a given node iff the following conditions are
met:

1. They are attached to the node using the "rule" relation,
i.e., (<node> rule <rule-node>). A given node may have an arbitrary
number of rule relations; all of them are run.

2. They are part of the global rule pool, and the node contains a link
to the pool, using the relation global-rule-pool. 

Edges, objects, local rules, global rules

In [C&C] the H machine is summarized using the standard definition of a
hypergraph, as a set of subsets of some universe U. 

Unordered sets are a good way to reason about hypergraph properties,
as familiar operations such as union and intersection, and their
mathematical properties, are applicable and useful. However, this form
introduces fairly severe performance limits.

To alleviate this situation, an edge in H is defined as an *ordered
multiset*. For example, the edge (book color green) is not the same as (book
green color), and (x next x) is a legal edge, although x is repeated.
In section [xxx] we describe some of the formal aspects of this set of
definitions and how they help with optimization.

While a edge is simply a list of nodes, other conventions are employed
to help with comprehension and performance. Generally, we attempt to
build as little as possible special handling of these conventions into
the kernel; however some is inevitable in order to get a reasonably
practical system.

The most common convention is the use of a three-node edge as an
object-attribute-value triplet. System conventions, such as the
denotation of attached rules, utilize this basic model. In graph
diagrams, this case is presented in node-labeled-edge-node style,
which is traditional.

A variant on this is the two-node object-property edge. This is usetul
when there is little utility to adding a value, such as for some
boolean properties. For instance we can say

(1 odd)
(2 even)

to denote membership in a set of odd or even numbers, respectively.

Another variant is a four-node edge which represents dataflow block. So one might say

(input1 input2 and-gate output) 

to represent a logic circuit fragment. Recall this is a convention,
not defined system semantics. The only interpretation in this case is
in diagram generation. When the following edge is defined:

(and-gate two-input-op)

The diagram generator knows to emit a gate icon and connect it
appropriately.

Most nodes, when created, are presumed to be "objects", in that they
will be expected to appear in the first position of an edge, and will
be executed. [What "most" means here is a bit complicated and needs
refinement. [explain]]

A basic principle of the H machine is that actions only occur due to
connections found in the evolving graph. No rule is found, for
example, in some hidden kernel "oracle" which magically knows of their
existence. Every object node, then, has certain system-conventional
links, in particular to sets of rules.

Global rules are attached to an object via a pool. The pool is
global-rule-pool-node, and rules are found under the grp-rule
attribute. So the global rule pool looks like this:

(global-rule-pool-node grp-rule rule1)
(global-rule-pool-node grp-rule rule2)
...

Each object then has a link to the pool:

(<obj-node> global-rule-pool global-rule-pool-node)

When a rule is defined, it is placed in the global rule pool (attached
to the global-rule-pool-node), unless the (local) flag is present in
the rule definition.

Local rules are attached to an object using the "rule" attribute:

(<obj-node> rule rule1)
(<obj-node> rule rule2)
...

Nodes may be populated with local rules by propagating them from one
object to another. This leaves the question of their
initialization. For this we define a local rule pool. It is
distingusihed from the global rule pool in that no default link is
created from nodes to the local rule pool, and there is no execuition
semantics associated with the local rule pool. Instead, the local rule
pool exists purely as a way to match against the set of rules,
typically by name. We might see, for example,

(rule
 (name next-rule)
 (local)
 (pred
  (?x next ?y)
  (?x local-rule-pool ?p)
  (?p lrp-rule ?next-rule)
  (?next-rule name next-rule))
 (add
  ...other edges for ?x and ?y...
  (?y rule ?next-rule)))

In this case the rule propagates itself down a chain of next
relations.

This technique of propagation is essential to controlling rules, both
from the parallel and sequential viewpoints. While it may seem clumsy
to add rule propagator matching to all rules of interest, rule
mutation allows us to modularize this: A new rule is defined, which
matches on the original rule, and adds pred/add edges to the original
rule as illustrated above. So the original rule remains syntactically
clean. We'll see in following sections application of this technique.

To summarize, then, each rule is added to the global-rule-pool-node
unless it has the (local) flag present. Rules are always added to the
local-rule-pool-node and are available then for matching and
propagation. One can say attach-to a node in a rule, in which case the
rule is local, in the local rule pool, and attached as a rule to the
given node. Any number of attach-to's may be supplied.

The global-node is often used as an attachment point, since it is
specially treated when using the primary execution function in the H
machine interpreter. Specifically, the global-node is executed
first (and later as well -- the execution loop is described in seciton
[xxx]).  This allows rules which are for initialization, i.e., rules
which create data or rules, to run and get things started.

Execution Loop

When a rule is run, and matches are found, new nodes and edges are
produced. All nodes affected by the created edges (whether old or new)
are placed on a queue for execution. In this way we propagate
execution in a local neighborhood.

[Since we're using hypergraphs, this is not as "local" as it might
seem. For example, by default all "attribute" nodes are expanded, so
some explosion is to be expected. See section [xxx] for discussion of
performance issues.]

Ideally, the queue should account for all execution needs. However,
especially when deletion is used, this cannot always be
guaranteed. The basic loop normally used, then, is one which tries to
compensate for shortcomings in the methods as currently developed.

The loop is as follows:

execute-global-all-objs-loop:
	exec-until-no-new-edges:
		exec-until-no-new-edges:
			execute global-node
		execute queue
		execute all-objs

This executes from the queue, global node, or across all nodes, until
no new edges are produced.

Rule matching and edge production

Rules have one purpose, and that is to produce edges. The predicate
pred specifies the subgraph which needs to match. Variables are
denoted with a leading question mark.  Note that, to the extent
possible, we have tried to stick to the rule that variables are only
an opitmization. That is, that main driving algorithm of the H-machine
is subgraph isomorphism, and although we have made "the usual"
modifications regarding ordering and duplication, variables are not
inherently necessary, except to make matching tractable. A consequence
of this is that variables can be used in rules and meta-rules with
well-defined meaning.

Rule semantics then are as follows:

The pred subgraph of a rule r is matched against a set of edges E, and
subgraph of G, normally supplied as some neighborhood found by
expanding the surrounding edges of the executed node. How the expansion
is done is described in section [xxx].

- All possible matches of r to a subset of E are found. The effect of
this is to bind variables found in the pred to some nodes in E. Each
such binding is then used, in turn, to add (or delete) edges as
determined by the add and del clauses.

- Variables in the add clause are substituted with the bindings found
by the pred match. These new edges are then added to G. Similarly, the
del clause substitutions result in edges which are deleted from G.

An example:

(rule
 (pred
  (?x owns ?y))
 (add
  (?x paid-for ?y)))

applied to

(john owns ford)
(john owns stove)

produces

(john paid-for ford)
(john paid-for stove)

Some handy built-in variables are bound during the execution of the
add and del clauses:

?this-obj

Bound to the node on which the rule was executed. 

?this-rule
?this-rule-name

Bound to the rule and rule name which matched and generated the edges.

?root-var

Bound to the root var found in the rule. 

These are handy for diag printing, or for passing/deleting rules
without a lot of extra fuss. For example one idiom used reasonably
often is

(del
 (?this-obj rule ?this-rule))

which deletes the rule that just ran from the object -- when you know
it's done, it can be disposed of and thus not run redundantly.

As mentioned above, rules have one purpose, and that is to produce
edges. Edges produced can include new nodes.  In some idealized
models (as mused about in [C&C]) one might require that new nodes are
never produced, and one simply uses a countable pool of pre-existing
nodes. It seems possible to formalize such a model, but it does not
seem practical. So in the H machine we adopt a notation to create
nodes.

However in the spirit of this Platonic view that nodes all really
exist already, we denote new nodes in the *predicate* (pred) part of
the rule. The relation is new-node, and is specially detected by the
kernel.

A quick example will set the stage:

(rule
 (name add-link)
 (pred
  (?x next ?y)
  (?n new-node sn1))
 (add
  (?y next ?n)))

In this case we're saying in essence that if you match against this
new-node edge, you get a new node. Plato springs to life! The
sn<number> syntax is special, and causes creation of an actual new
node (n<number>), which can then be used in the add clause.

[In fact, a node of the form nn<number> is created by the rule
definer, and it in turn causes a new node to be created at run time.]

A rule can specify any number of new-node relations, the nn<number>
designations are considered scoped within their defined rule. The
variable to which the new node is bound is arbitrary (?n in the above
example).

Printing

The print edge has built-in semantics. It iss useful for tracking and
debugging. Simply, any edge of the form:

(print <node> ...)

will be printed when added.

Diagrams

Graphs have a natural graphical interpretation; hypergraphs are a bit
harder to display.  For the H-machine we have a set of drawing
interpretations and heuristics. These are imperfect but illustrate the
system. Simple heuristics are used for edges of 2, 3, and 4 nodes,
coupled with meta-information from G.

Three-node edges are by default drawn as node-attribute-value, where
the attribute is a labeled arc.

In rules, edges in the pred clause are black, and edges in the add
clause are red. Note that in this document we use dotted edges in some
diagrams rather than red.  Edges in rules which are in the pred and in
the del clause are marked in blue, with an "X".

Color is used primarily to make the diagrams nicer, but also to help
illustrate the information-passing properties of the propagation
rules. More on this in section [xxx]; here we will explain the color
objects and what they do.

A fixed set of colors is added to the system via the color-circle-data
rule. This is literal data and forms a color circle (like a "color
wheel", although I don't believe it strictly follows the traditional
volor order), each node of which is named for a color and is related to the
next color by the next-color relation.

The names of the color nodes are chosen to be the same as the color
names supported by graphviz (gv) (see [gv]). Therefore in a gv dump,
using the node name gets the correct color with no extra effort.

By default, a given node will be colored according to its color
relation. So due to

(x color red)

the node x will appear in the gv image colored red.

As an example of the flexibility as well as cuteness of the H
language, to get the color circle itself to be displayed with the
correct color, it needed an attribute to a node with its color name,
i.e., itself. So this rule does the trick:

(rule
 (name color-color)
 (attach-to global-node)
 (root-var global-node)
 (pred
  (global-node next-color)
  (?x next-color ?y))
 (add
  (print color-color ?x)
  (?x color ?x))
 (del
  (?this-obj rule ?this-rule)))

This says, first, that it's a global-node rule, which means it will be
executed up-front. Also, when done, we know we will have filled in all
the color nodes, so it deletes itself from the global node. For the
rest, it knows ?x is a color node because it has a next-color
relative. It then simply adds the color link to itself.

Note that by simply stating the next-color relation, we take care of
all the color-circle nodes. There is no need for an
explicit "iteration" indicator. This aspect of the H-machine is very
powerful, but also delicate, since combinatorial explosion must be
contained.

fe-rule-test uses the rule-copying rules, thus illustrating purely
parallel propagation. Copying is defined completely within the rule
system itself.


----------------------------------------------------------
11/21/14

We attempt to remain as true to pure hypergraph iso as possible. For
performance purposes we constrain the graphs as follows:
		
	- Edges are ordered, i.e., (a b c) is not equal to (c b a)
		However, each edge is still a set, so we cannot have repeated
		nodes; hence e.g. (a b a) is not permitted. Thus each edge is
		a strict total order.

The fact that we are using sets, though ordered, means that for iso
there must be a 1-1 correspondence between rule nodes and object
nodes. Thus two rule nodes (vars) cannot map to the same object node,
just as perforce a single rule node (var) cannot map to more than one
object node.

Being as true as possible to pure hypergraph iso is important, but
it's worth pointing out that pure iso seems to introduce an
interesting variation on propositional logic, e.g.:

p(x,x) is not expressible, as say in next(x,x) (or x --next--> x) to
denote a self-loop. Instead in the hypergraph ordered-set (list)
syntax:

(x p) 

is equiv to the standard tuple form

(x,p,x)

[4/20/16: Note relaxed this condition later and allowed explicit self-loops.]

----------------------------------------------------------
12/9/14

Compilation and having "primitive rules" or "primitive objects"
or "primitive subgraphs" of some kind are distinctly
different. Compilation is analogy/isomorphism based, in the sense that
we can transform domains and execute in a "better" (e.g., faster)
domain, then transform back.

Basic graph xform alg is to compare every subgraph with every other
subgraph. So *subgraphs* interact, and one must be a rule.

----------------------------------------------------------
10/25/22 -- Efficiency measures, updated

Recently changed the measures to be more mathematically straightforward.

Start with this basic relation:

Tested = matched-new-edges + matched-no-new-edges + failures

Thus if we use Tested as the common denominator we get measures that
add to unity. However then redundancy is based on tested, not
matched. This is a little odd but makes the measures make more sense,
e.g., below the three measures add to 100%. However with the new definition
of redundancy it can never be 100%.

efficiency% = (matched-new-edges/tested)*100
redundancy% = (matched-no-new-edges/tested)*100
failure% = (failured/tested)*100

rule-stats:

name                      tested    matched   new-e     not-new-e failed    max-expand-len  efficiency% redundancy% failure%
RULE-30-ZERO-RULE-1-1     13        10        9         1         3         70              69.23       7.69        23.08

3/20/16 -- Efficiency measures [original]

Fundamental to the performance of the system is the rate of edge
production. This in turn is governed by the production of edges
resulting from each execution (match-end-execute) of a rule against
some object.

Rule cumulative Effectiveness defined as produced-edges/tested. Unity is best.

Call this E(t)

Overall we have rule.(tested,matched,produced-edges), where each set
is inclusive of the one to its right in this lexical order.

Cumulative Redundancy is produced-edges/matched. Again unity is best.		[Note changed this def in note below 3/12/17]

Call this R(t)

If E(t) is flat, i.e., dE(t)/dt = 0, for some period, then suspend rule.

3/12/17: Finally added something on this to the perf
stats. me-efficiency ("me" stands for "match-and-execute-rule") is
new-edges/tested, i.e. the fraction of rules which produced new edges
for each rule tested. Note new-edges means number of rules not number
of edges. Higher values are better, with unity meaning perfect
efficiency, i.e., every rule tested produces new edges.

me-redundancy is an indicator the number of rule tests "wasted", i.e.,
which matched but did not produce new edges. The common case of this
that earlier matches produced edges, and we thus have a duplicate test
and match. (An empty add clause can also cause this, but that will
rarely or never occur.) me-redundancy is not-new-edges/matched, where
not-new-edges is the number of rules which matched and did not produce
new edges, and matched is the number of rules which matched. So for
me-redundancy unity is the worst -- every matched edge failed to
produce edges, and lower numbers are better.

A piece of perf-stats output from a typical fft run is:

ME-EFFICIENCY                      0.0072660567         1              0.0072660567
ME-REDUNDANCY                      0.8164639            1              0.8164639

This shows we have a long way to go in optimizing the system.

;; We used to calc it this way, and in some sense it's better than not-new-edges/tested, since it
;; gives us a figure not dependent on the failure rate.  We're using not-new-edges/tested since we can
;; then say eff%+redun%+fail%=1
;;
;; (setf (timerec-sum red) (div (float not-new-edges) (float matched)))

----------------------------------------------------------
4/5/16

Good to split the delta and opt stuff out, and provide optimizers
external to the standard global rules. But items like randgen and
colorgen that rely on deletion only work under the optmimized
model. In a global model they re-execute and loop.

Illustration of the notion of "applicative" programs in the context of
the H language. With color, we moved to a tree-color-wheel model, no
del.

This was in hnew.lisp (moved here 4/5/16):

		  ;; Global color generator, based on moving val through a
		  ;; color wheel. Due to the of use del this is not preferred, and
		  ;; use of the color tree is encouraged instead.  In use one
		  ;; just makes (<obj> colorgen) and this rule will create
		  ;; (<obj> color <some-color>)
		  ;;
		  ;; If reactivate, remember that this rule needs to be
		  ;; propagated, can't be global. The latter is one of th
		  ;; issues.

		  (rule
		   (name colorgen)
		   (local)
		   (root-var ?x)
		   (pred
			(?x colorgen)
			(colorgen val ?c)
			(?c next-color ?d))
		   (add
			(print ?x ?c)
			(?x color ?c)
			(colorgen val ?d))
		   (del
			(colorgen val ?c)
			(print ?x ?c)
			(?this-obj rule ?this-rule)))

----------------------------------------------------------
11/23/16 -- Qets: Ordered mulitsets, i.e. tuples:

[5/21/23: For update, see Qets, Seqs, and Strings, oh my!]

Edges in H are really tuples (lists). Most set operations work, but
it's not very formal. Matching works well without an explicit set
formulation, but subsetting is not really correct, and we need a good
method for the optimizer.

Although we can use the term "tuple", the properties we need are
sufficiently unconventional that we define a new term "qet" (rather
than "set"). We can then talk about a subqet and so forth.

I went through a number of names before landing on qet. Qet is
"sequential set". All other ordered-set terms I could think of, like
list, sequence, or tuple, imply that a "sub" unit is a contiguous
piece. A subset of a true ordered set (i.e., no dups) is the closest I
can get, but then we need to extend that to multiset, and the
terminology breaks down.

In the end I'm not trying to define new mathematics but looking for
good class and var names for the code, to avoid confusion.

Intersection on sets can be defined as the largest common subset
between two sets. It can be shown that the subset relation on the
powerset forms a lattice, and that thus there exists a unique such
set. However this definition does not hold for qets, and itersection
is not necessarily unique. E.g, (1 2 a b 3 4) qet-intersect (5 6 b a 7
8) could be (a) or (b).


qet-intersect (qi) definition:

x qi y == - Let x1, y1 be the removal of  all elements not common to x and y from x and y
  	   	  - Retain the order of the remaining elements
		  - The largest common subsequence is then the qi. There may be more than one of the largest size

Thus, we abandon intesection as a model and just talk of common
subsets, and the intersection/union (meet/join) lattice becomes a
partial order where meet is not unique, i.e., there is no unique
common lower bound.


We define the following properties:

- The elements of a qet are atomic, i.e., they are nodes in the graph
  G.  In the Lisp world that means symbols, strings, and numbers. So
  there is no notion of a qet of qets.

- Unique intersection is not defined. Looking at intersection as the
  common subset between two sets, we can say that qet intersection is
  like set intersection where the set of possible common subsets is
  not unique.

- First define a subqet of a qet q, analogously to a subset, as any
  subset of q when considered as a multiset, while retaining order. So
  for instance:

	subqets((1 1 2 3)) = {(1)(2)(3)(1 1)(1 2)(1 3)(2 3)(1 1 2)(1 2 3)(1 1 2 3)}

  A simple derivation shows that if the elements of the qet of size n
  are unique, then the number of subqets is 2^n, i.e., the same as the
  number of subsets.

- Given an edge (a qet), we can construct the lattice of subqets
  fairly easily.

----------------------------------------------------------
12/28/16  -- Failure Processing

At this time I removed failure status edges from the H kernel, because
while the use of failure was successful, it was costly just
accumulating edges. If we bring it back, it will have to be with some
type of optimization, say that assures any rules triggered are so
done very soon after the failure, so the entries may be gc'ed quickly.

The primary use of failure was as an efficient way to determine that
the end of an exec-rule path had been reached: if one propagates rules
and their execution exactly, then they should all succeed until the
end of the graph pathway in the computation. In particular, one could
avoid transmitting and allowing it to fail on many nodes. Other
solutions are possible, just with more edge propagation.

I'm still using success successfuly and will continue since it looks
solid and allows good sequential optimization.


----------------------------------------------------------
1/28/17 -- Original Hoss notes from the top of the file

#|

g -- closure

(funcall g <method> <args>) == calls <method> by looking up name in g

(defm f) defines a local fcn f and adds to lookup table in g. 

Basic scheme-like model omits the global def of a method and allows std functional eval:

					local:					(f x)
					external:				((g f) x)

Even in scheme one needs to vary from std eval in that the method
names want to be unquoted. One could bind global vars to their own
names or eqv but then this pollutes the global name space. So macros
are chosen for this.

In CL, define a macro ! to represent the above:

					local:					(f x)
					external:				(! (g f) x)

Compare to C++  dot syntax x.f().g(42)    ((((x f)) g) 42)

11/17/22 Addendum to dot syntax matters: Implemented a symbol-based
dot-syntax detector at the symbol level. This runs as a pass in
hoss. Any non-number symbol with a dot in it is transformed as
follows:

		(x.y z) ==> (! (x y) z)

This is cute, but doesn't nest well. You can't say eg ((x.y z).p
q). This would reuire a mod to the Lisp system's basic read syntax,
and does not seem to be possible with readmacros.

In the end I think my favorite model for this in Lisp is ((x y) z),
like Scheme. So I'm not going to start using this but I may leave the
code in hoss tied off in case I want to bring back some variant later.

Superclass handling:

1/26/17: Implemented the following:

Each defm produces two methods, one for local access and one for
class-level access. So for example:

(defc c nil nil
  (let ()
	(defm f (x) ...)))

=>
... (labels ((f (x) ...)
			 (c-f (x) ...))
			...)
	...export both f and c-f...

So now one can override in a subclass and call the super easily. This
is the main use case.  Although the class method is exported as usual,
it's not expected much use of that will be made.

Considered other forms. The C++ expression x.c::f(y) in Scheme syntax
could be:

	((x f c) y)
	((x f super) y)
	((x c f) y)
	((x super f) y)

These don't offer much over the naming method.


See top-obj.lisp -- all defc's inherit from it
	Defines self, get-self(), set-self), and init()

	top-obj is the top class, automatically inserted when a superclass
	is nil. It is the only class known by the hoss kernel and is used
	for setting self and for the init calls.


;; I considered omitting the defm below, however defm is a handy
;; visual marker and also helps scan the class def.

;; Note to support trace can't return-from a method (block or local fcn ok)

(defc class-name superclass-name (cl-arg-spec)
  (let ((a 1)
		(b 2))
	(let ((c a))
	  (defm m1 (x) (+ a x))
	  (defm m2 (x y) (* x y b)))))

(defun make-obj (cl-arg-spec)
  (let ((method-lookup (make-hash-table :test #'eq)))
	(let ((a 10)
		  (b 20))
	  (labels ((m1 (x &key (y 100)) (+ x y a))
			   (m2 (x) (* x b))
			   (m3 (x y) (f x) (+ x y))
			   (m3t (x y)
				   (print (list 'enter 'mt x y))
				   (let ((r (let () (f x) (+ x y))))
					 (print (list 'exit 'mt r))
					 r)))
			  (setf (gethash 'm1 method-lookup) (function m1))
			  (setf (gethash 'm2 method-lookup) (function m2))
			  (lambda (mname)
				(let ((m (gethash mname method-lookup)))
				  (lambda (x)
					(apply m x))))))))

;; Inherited

(defun make-obj ()
  (let ((method-lookup (make-hash-table :test #'eq)))
	(let ((a 10)
		  (b 20))
	  (labels ((m1 (x &key (y 100)) (+ x y a))
			   (m2 (x) (* x b)))
			  (setf (gethash 'm1 method-lookup) (function m1))
			  (setf (gethash 'm2 method-lookup) (function m2))
			  (let ((c 30)
					(d 40))
				(labels ((m3 )
						 (m4 ))
						(setf (gethash .... method-lookup))
						;; ...
						(lambda (mname)
						  (let ((m (gethash mname method-lookup)))
							(lambda (x)
							  (apply m x))))))))))

;; Convenient formatting def
;;   defr -- "def recursive"
;;   defl -- "def local"

(defr
  (defl f (x) ...)
  (defl g (y) ...)
  ...code...)  =>

(labels ((f (x) ...)
		 (g (y) ...))
   ...code...)

|#

----------------------------------------------------------------------------------
9/12/22 -- Began these Hoss doc and notes, beyond the file notes immediately above

Override/"Virtual" Methods
--------------------------
Superclass methods maybe overridden, ala "virtual functions" but the
call site needs to be object-invoked, i.e., (! (self f) x) rather than
just (f x). Provide automatic way to do this, via marking the method
"overridable"?  Not clear there is a major benefit except the syntax
of the call.  We already know the ! form is quite clunky, so it seems
that trying to cure that is more appropriate. This would seem to
entail introducing a dot-syntax notation, so we can say (x.method 1 2
...). Needs more thought.

Class constructor args
----------------------
New model combines arg lists from the class inheritance
chain. Effectively, we only allow required and keyword args. The lower
make functions accept a list of args from the current class and go in
bottom-up, left-to-right order, gathering all the required args in
that manner, and gethering all keyword args at the end of the arg
list.

&optional and &rest won't work but they are not detected. Similarly
duplicate vars are not detected.  Use of these constructs likely will
result in errors downstream.

Example:

(defc x nil (a b &key c)
  (let ()
	(defm f ()
	  (list a b c))))

(defc y x (d e f &key (g 42) h)
  (let ()
	(defm g ()
	  (list a b c d e f g  h))))

(defc z y (i j)
  (let ()
	(defm h ()
	  (list a b c d e f g h i j))))

#'make-x
#<FUNCTION MAKE-X (A B &KEY C) ...
#'make-y
#<FUNCTION MAKE-Y (D E F A B &KEY (G 42) H C)...
#'make-z
#<FUNCTION MAKE-Z (I J D E F A B &KEY (G 42) H C)

This seems to work pretty well. Wave equation stuff from poly.lisp:

(defc differential-equation nil (nsize msize file c &key 2d (mfrac 0) (yscale 1000.0) (unit-pulse-loc 0))
(defc euler-bernoulli-beam-equation differential-equation ()
(defc wave-equation differential-equation ()
(defc damped-wave-equation differential-equation (&key (damping-coeff 0))

The damped class adds in the damping coeff, and we can pass it in with all the other args, e.g.:

(let ((msize 80))
  (setq x (make-damped-wave-equation 1000000 msize
									 "x55.wav"
									 .999
									 :yscale 1e3
									 :mfrac 0
									 :unit-pulse-loc (/ msize 10)
									 :damping-coeff 5e-5))
  (! (x run)))

Nested defms
------------
10/17/22

Fairly major basic change to hoss, where one can nest methods in a let
scope, to allow for local state without adding to the object's
top-level env. This is helpful e.g. in defining cache tables which may
then just be cleared rather than recreated, textually local to where
the tables are used.

The main drawvback is that ref to the method name from outside the let
block must be explicitly OO, e.g., via self.

However the basic existing funcitonality remains the same so there is
no drawback unless the feature is used. Even then the overhead is
often low relative to the savings in GC, etc.

(defc x nil nil
  (let ((a 10))
	(defm f (x)
	  (if (= x 0)
		  1
		  (! (self g) x)))
	(let ((b 2))
	  (defm g (x)
		(+ x b (! (self f) 0))))))

[24]> (setq x (make-x))
#<FUNCTION :LAMBDA (MNAME) (LET ((M (HGETHASH MNAME METHOD-LOOKUP-HASH))) (LAMBDA (X) (APPLY M X)))>
[25]> (! (x f) 10)
13
[26]> (! (x g) 10)
13
[27]> 

This is somewhat interesting in that each method in a class can be in
its own closure in a tree of envs, each part of the same object. The
method refers to the local scope and higher.

----------------------------------------------------------
2/15/17 -- Got rid of partial-match code based on envs and cross-aux2

It's not practical since a partial match can be a very large number of
edges. So the h.2.15.17.lisp file has the old code in it.

As with the order of scanning envs in cross-aux2, partial match is
useful only if one gauges the selectivity properly, or edge explosion
results.

I'll keep the e- functions around for now since they're
expository. There is much room for hope here.

----------------------------------------------------------
2/20/17 -- Some as of now concluded experiments with tracking new
		   nodes, last-matched edges, etc.

./h.archive/h.2.26.17/h.2.20.17.lisp contains code for tracking the
addition of edges in order, including tracking deletions. Given an
edge one can find the next or prev edge in the creation order. Of
course can't hold on to a deleted one, and I ran into that issue.

One experiment was to try new-edges-only if the previous match of a
rule was successful. This doesn't work because we might have several
"new" edges which match against a background of "fixed" info, and
happen to take the newer edge first, stranding the old one.

Another experiment is to disable a rule if >N edges have been produced
since its last edge-producing match. This requires the ordering to be
working well, and really needs to be redesigned as a btree to do it
right. This is not the first time I exprimented with throwing out
rules heuristcially, and none has worked out. The method here may have
promise, and I might return to something like it.

----------------------------------------------------------
2/26/17 -- Abandoned the exec model of rule control

Lots of trouble just finding the source of problems with sequence and
control, e.g., the battle to reduce the number of tests to
od-next. This causes me to rethink the goals of the exec model,
including record rule success/failure (failure was pulled out a little
while back), which was to provide a framework for automated sequential
optimization.

I was able to remove the use of rule success fairly easily, just by
replacing the predicate which detected sucess with appropriate
conditions and context. And exec has issues in that it's very hard to
get it to work by hand.

Also, there has been success in optimizing rules by rule modifcation
or generation, so furhter automation will look at that as well as
other ideas, e.g., building dependency graphs and using inductive
learning to get the right set of rule mods and propagations.


h.2.26.17 contains the last version which supported the associated
operations, like exec and success recording.

----------------------------------------------------------
3/10/17 -- Notes on the kernel for the doc

Need to represent data and rules in the graph, so the system is
closed. Thus elem0, etc.  The kernel needs to know about this and has
special knowledge to read the rule form in G and convert it into an
internal representation for processing. This representation of course
is just another hypergraph, but outside of G.

One goal of the H-machine is to devise a means to execute rule-driven,
spreading-activation models like this with reasonable computational
complexity. For the general interpreter, this would be heuristic,
since it's clear that quadratic behavior or more is required for full
generality. This goal has not yet been achieved. 

Another approach is through compilation. Again, that needs to be
defined precisely; however we have shown that one can use rules to go
easily from a top-down form to a bottom-up form, and this likely forms
the basis of compilation. 

[See ref to here from 8/5/2020]
At least one aspect of compilation may be usefully thought of as
a "reduction of dimensionality", i.e., as in the type evaluation and
inference found in modern functional languages. In the case of the
H-machine, this could manifest itself as the partial matching of
rules, which then generate other rules that are more
closely "attached" to required objects, since they have been bound
already. In theory, with enough objects bound, the rule could be
replaced by a compiled "procedure", which acts immediately up seeing a
new edge of the appropriate type added to the system.

-- Special Edges --

[Here for
  - The "philosophy" of it. 
  - new-node
  - rule interpretation]
   

[I had experimented with more special edges, including ones for
recording rule executions and affecting execution. These did not prove
palatable, and I removed them. Instead, rule mutation and related
optimizations look more like what's needed. I suspect we'll be led to
a model where some sort of induction algorithm runs on rule traces
using global rules, and moves the rules to be local based on stats
feedback. But there are many other methods to consider.]

[There is also the temptation to install some kind of "primitive"
behavior under the covers, as a shortcut for building it up somehow
with a pure H-machine model. In similar systems, arithmetic operations
are typically the first to be moved down in this way. We resist this
temptation, in favor a more pure approach.]

I used Hoss throughout and that has proven valuable.

A primary goal of the kernel design is to keep as much specialized
knowledge as possible out. I avoided the temptation to add bells and
whistles that would satisfy the urge to have something run faster, or
short-circuit something for the sake of displays and such. Most such
knowledge is in the graphics dumper, which is written more or less
conventionally, taking parameters from the edge system. However I
don’t think too much leaks through.

Performance is a major issue, and I have not solve the primary
complexity problem, combinatorial exp


----------------------------------------------------------
3/17/17 -- Modularity

Using the H-machine for awhile as a programming language, there are
some interesting notions of modularity which have emerged. 

First is the observation that the use of isomorphism/matching allows
computation in what are effectively scoped contexts, without
explicitly defining a scoping model. For instance I can use the
attribute names "elem" or "next" in many objects and they do not
conflict, as long as I narrow down the use as appropriate, i.e., by
using other attributes to isolate the unique set of subgraphs intended
to match. Sometimes one can assign a unique "type name" or similar
explicit tag to refine these attributes, but it's not required
(although good "engineering practice" may call for a universal set of
such tags in a given project).

A side benefit is that if one can match on these attributes as well,
and find if desired for example, all edges which use a given
attribute. However, one area where this has become an issue is in the
graph display dump, which is partly driven by attribute name. One may
expose a "next" attribute, for instance, and get more nodes and edges
dumped than desired. The solution I have so far for this is simply to
added more edges which denote attributes for display purposes
only. Doing this is mitigated some some degree by the following.

The second notion of modularity that has emerged is similar to that
found in Aspect Oriented Programming [Kicksales]. In the H-machine,
rules are distinct entities and do not need to be included in some
lexical scope. [Nested rules offer lexical scoping; however this is
really just syntactic sugar for more complex manipulation of edges
which one must do instead.] In addition, rules can match on and modify
other rules. This supplies the capability to alter system behavior
across a wide range using rules and meta-rules expressed outdide the
modules affected.

A good exmaple of this is in the rule file fft-delta.lisp. Here rules
match on the fact that an fft relation exists.  From there colors are
defined; random values are assigned by matching against the rule-30
nodes; based on the randomness mutations to the data flow graph are
produced; and a few edges added solely for display purposes.

Another rule matches on the fft rule itself, changes it from global to
local, and adds rule-propagation clauses. This rule thus optimizes the
fft-rule, which is written in a "clean" style.

These rules act on the result of the fft and related rules, while
those rules remain fixed, at least in the source code, and don't refer
to any of the attributes, such as colors or random values, added by
the rule in fft-delta.lisp.

----------------------------------------------------------
4/5/17 -- Conclusion to the main paper

Summary
TBD.

-- Open-loop systems

One of the traditional attactions of rule-based systems has been their
declarative nature, offering at least some degree of freedom from
control-structure concerns and a good way to abstract over data in a
modular way. The H language is no different, and experience with it
has shown that interesting things can be produced using this model.

And as with other rule-based systems, performance is a challenge. Many
approaches are possible, and many have been tried [ref Rete, ...]. 

Both the H-Machine and the H language are meant to have an "open-loop
with feedback" flavor about them, in that the default behavior is to
try a large number of cases, e.g., an n^2 pass. This is very powerful
from an expressive point of view, but needs to be controlled. 



Kernel

[Should have more than zero, but it's already too long. I may have
a paragraph, then refer to another paper.]

Compilation 

I have a few thoughts on compilation, but have not gelled them well
enough to try anything.

Generating dataflow graphs is one good way to look at compilation. We
can contemplate a compile-to-hardware sort of model, where on a
sequential machine the dataflow graph is evaluated sequentially,
bottom-up – a standard compilation model.

But there is also the question of just what gets compiled. The rule
system has a very basic semantics, but is very hard to optimize. We
don’t first have questions about how to generate code, but how to
control rules. I have tried several basic heuristics, but have not
landed on anything definitive. However the ideas herein, especially
rule generation and modification, look like they are an important part
of an optimization strategy.

The other major optimization problem emerges in the matching
algorithm. Hypergraph subgraph isomorphism is non-trivial. However
there is much literature and I have only explored some of it. Again, I
use basic heuristics to limit explosion but it has issues with large
runs.

One general idea concerns rule specialization and how this maps to
types. A rule is a graph which imposes constraints on the set of
subgraphs which may be selected from a larger graph. The more
constraints (edges) the rule has, the more cons…blah blah,,,

[Compilation can be viewed as an isomorphism problem, where we
transform from one domain to another, evaluate, and transform back.]

Bootstrapping

----------------------------------------------------------
5/25/17 -- Notes on submission to IEEE ICRC 2017

	5/15/17: Submitted abstract from version xhmachine-ieee.5.25.17.doc
	
	6/7/17: Submitted xhmachine-ieee.pdf, saved as xhmachine-ieee.6.7.17.pdf
				Also saved xhmachine-ieee.doc to xhmachine-ieee.6.7.17.doc

	8/11/17: Freeze xhmachine-ieee doc, since rejected.

	8/13/17: Submitted poster entry, hmachine-poster-ieee.pdf
	8/30/17+: No response wrt poster submission

Abstract used for poster submission:

The H-Machine: Experiments in the Organic Growth of Interacting Data Structures

The H-Machine is a hypergraph-based language and interpreter, based on rule matching by hypergraph isomorphism. Rules are part of the hypergraph being constructed, and can be matched and modified as can other data, thus forming a complete meta-system. The H-Machine language is extremely simple, yet can express a wide range of data structures and computations, with inherent parallelism. The matching system allows a full range of expression of recursive relations, implicit iteration, and, via meta-rules, a modular way to describe computation at a high level.

The language and an implementation are described, and the capabilities of the H-Machine by extended example: Building a bottom-up FFT butterfly dataflow network organically, starting from the top-down recursive equations; using meta-rules to generate rules for compactness of expression and to modify, copy, and propagate other rules for optimization; and modifying behavior using simple modular expressions which specify the interaction among data structures.

Data and rules have a simple graphical interpretation which offers good visibility into the resulting structures. The examples illustrated show both data and rule structures, derived data flow graphs, cellular automaton matrices, ancillary relations, and rules – predicate subgraph, edges to be added, edges to be deleted -- within a single graphical model, The Graphviz toolkit is used for rendering, and all graphics are generated by the system. Links to a full paper, code, and a graphics gallery of generated H-Machine runs are included.

10/10/17, a reviewer's response:

This is an interesting paper. It presents the H-Machine which is a
system for computing via graph rewriting rules based on matching
isomorphic subgraphs, and providing transformations on that graph. In
some way there is an analogy to advanced language macro systems, but
the focus is on naturally expanding and possibly recursive
matches. This provides some notion of iteration, without iteration,
and seems to provide the foundation for some form of computation (in
the same sense that game of life denotes computation). It dives into
examples from the computational structure of an FFT program. In this
example, it traces out the dataflow through the program. Higher order
forms of transformation rules (meta-rules) are introduced that make
complicated transformations more terse and expressive. Though this is
an interesting paper, I could not convince myself that this approach
represents a promising future form of computation. It feels more like
a significant intellectual exercise in understanding a form of
abstraction and self-emergent properties. Computation based on
efficient graph isomorphism (despite the two references to work in the
area) is not motivated as a viable. Some additional comments: - An
interesting aspect of this paper is exactly how close it is to macro
expansion in programming languages. The main difference is that macros
are named, thus avoiding the isomorphism issue. They also support
higher-order macro expansion which is analogous to meta-rules. This
raises the question, why is computation based on isomorphism more
interesting than one with explicitly-anotated expansion points more
interesting? Perhaps a question to address in a future version of this
paper. - I'd appreciate a somewhat less jargon-dense version of the
paper. Using terms like "forming a complete meta-system" in the
abstract is not likely to elicit much concrete understanding in your
audience. It isn't clear to me what a "subset of h" (a hypergraph) is,
as graphs don't often have a subset relation implicitly defined on
them. Background before jargon is, in my experiences, a better way to
capture an audience. - I prefer to avoid using color-based
descriptions since many people still print out papers in back and
white. Red is not a great color due to color blindness.


----------------------------------------------------------
9/9/17 -- Removal of print-gc and true print edges


----------------------------------------------------------
10/8/17 -- Using a type property

- Types, like vars, are "just an optimization", i.e., they have no intrinsic meaning

	;; Types are considered conjunctive, thus obj-edges must have all
	;; the types mentioned in the type-preds, but can have more as
	;; well.

- 4/27/18 Consider in light of new rule evaulation and the dimension
  idea, i.e., how many times an edge is used by a rule. Something like
  a type property raises the dimension.

----------------------------------------------------------
2/8/18 -- Crap

A rooted tree is clearly separable into levels
A (rooted) DAG is a tree with identical sub-branches collapsed into one
Therefore a DAG can be separated into levels
Any cycle in a tree or DAG must go back one or more levels

----------------------------------------------------------
2/15/18 -- Macro probably not needed, since mapcar and friends take multiple args

(defmacro dolists (params lists &rest body)
  (let ((temp-var-list (mapcar (lambda (x) (gentemp)) params)))
  `(do
	,(mapcar (lambda (x) `(t1 ,x (rest ,x)))
			 lists)
	((null t1) nil)
	(let ,(mapcar (lambda (x) `(,x (first t1)))
				  params)
	  ,@body))))

----------------------------------------------------------
2/26/18 -- Dumped attempt to be able to retract edges in env-triggered.

This never worked well, and resulted in loops. Much of the use of it
is commented-out anyway, so this is cleanup. See h.2.26.18.lisp.

It leaves the question of how we deal with already-triggered
rules. The presence of a new-node facility in the predicate means we
can't just rely on existing-edge detection.

----------------------------------------------------------
4/11/18 -- Notes on the different Common Lisps used here

clisp
Performance is good but not great.
Biggest problem is memory -- does not seem to allow sizes more than 2G.
Debugging faciitiles better than other CLs.



abcl -- Armed Bear Common Lisp
java -jar c:\Users\lstabile\abcl-bin-1.4.0\abcl.jar
Does not handle ^C interrupt -- exits. Thus it's only good for benchmarks.
Very fast CL.
4G max heap:
java -Xmx4000m -jar c:\Users\lstabile\abcl-bin-1.4.0\abcl.jar

Note java arrays are int-index, and this only postive 32-bit, so 2G is
largest number of elements possible in a single array.

I have set heaps large, e.g. 20g below:

		java -Xmx5g LasTest.java
		Hello World!
		1073741824
		42
		1073741824
		$ java -Xmx20g LasTest.java
		Hello World!
		1073741824
		42
		1073741824


Laptop:

ANT_HOME=C:\Users\lstab\apache-ant-1.10.14
JAVA_HOME=C:\Program Files\Java\jdk-21

Built abcl:
"c:\Program Files\Java\jdk-21\bin\java.exe" -jar c:\Users\lstab\abcl-src-1.9.2\dist\abcl.jar

To build abcl:
cd c:\Users\lstab\abcl-src-1.9.2
ant -f build.xml

Rosencrantz:

ANT_HOME=C:\Users\lstabile\apache-ant-1.10.14
JAVA_HOME=C:\Program Files\Java\jdk-21

Built abcl:
"c:/Program Files/Java/jdk-21/bin/java.exe" -Xmx4000m -jar c:/Users/lstabile/abcl-src-1.9.2/abcl/dist/abcl.jar

"c:\Program Files\Java\jdk-21\bin\java.exe" -Xmx4000m -jar c:\Users\lstabile\abcl-src-1.9.2\abcl\dist\abcl.jar

To build abcl:
cd c:/Users/lstabile/abcl-src-1.9.2/abcl
ant -f build.xml

Eclipse:

chmod 0 examples/java-interface due to erronous pickup of BankAccount example as main

"C:\Program Files\Java\jdk-21\bin\javaw.exe" -classpath "C:\Users\lstabile\abcl-src-1.9.2\dist\abcl.jar;C:\Users\lstabile\abcl-src-1.9.2\dist\abcl-contrib.jar" -m abcl/org.armedbear.lisp.Main

"C:\Program Files\Java\jdk-21\bin\javaw.exe" -classpath "C:\Users\lstabile\abcl-src-1.9.2\abcl\build\classes" org.armedbear.lisp.Main

- Had to import into default workspace to get around "module" error
- Had to set wd to hmachine
- Issue with redefined memq! Not in clisp; caused recursive loop and out of stack
- Need macro for impl-spec define, e.g. (when-cl-type :clisp (defun memwq ...))
- Due to eclipse us enow have two sources!


jdb:

Start with listening:
"C:\Program Files\Java\jdk-21\bin\java.exe" -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=1044 -classpath "C:\Users\lstabile\abcl-src-1.9.2\abcl\build\classes" org.armedbear.lisp.Main

Attach:
"C:\Program Files\Java\jdk-21\bin\jdb.exe" -connect com.sun.jdi.SocketAttach:hostname=localhost,port=1044

trace go methods 5614




sbcl -- Steel Bank Common Lisp

Looks likes it accepts very large heaps. See below. 
Emits many warnings on builds.
Considerably faster than clisp. Not clear if faster than abcl.

sbcl --help
Usage: sbcl [runtime-options] [toplevel-options] [user-options]
Common runtime options:
  --help                     Print this message and exit.
  --version                  Print version information and exit.
  --core <filename>          Use the specified core file instead of the default.
  --dynamic-space-size <MiB> Size of reserved dynamic space in megabytes.
  --control-stack-size <MiB> Size of reserved control stack in megabytes.

Common toplevel options:
  --sysinit <filename>       System-wide init-file to use instead of default.
  --userinit <filename>      Per-user init-file to use instead of default.
  --no-sysinit               Inhibit processing of any system-wide init-file.
  --no-userinit              Inhibit processing of any per-user init-file.
  --disable-debugger         Invoke sb-ext:disable-debugger.
  --noprint                  Run a Read-Eval Loop without printing results.
  --script [<filename>]      Skip #! line, disable debugger, avoid verbosity.
  --quit                     Exit with code 0 after option processing.
  --non-interactive          Sets both --quit and --disable-debugger.
Common toplevel options that are processed in order:
  --eval <form>              Form to eval when processing this option.
  --load <filename>          File to load when processing this option.

User options are not processed by SBCL. All runtime options must
appear before toplevel options, and all toplevel options must
appear before user options.

For more information please refer to the SBCL User Manual, which
should be installed along with SBCL, and is also available from the
website <http://www.sbcl.org/>.

c:\Users\lstabile\hmachine\src> sbcl --dynamic-space-size 3000

Ran sbcl --dynamic-space-size 8000, and used with 200-level rule-30 run.
		Failed!
Ran sbcl --dynamic-space-size 16000, and used with 200-level rule-30 run.
	Succeeded, and got a dump of y.gv (with center-up) and y1.gv
	(without center-up). Each was too big for neato, so no graphics.
  
----------------------------------------------------------
3/26/19 -- Rule copying



----------------------------------------------------------
8/5/2020 -- Dimension

Finally back to rule opt, and so forth! Really need to record
thoughts, code, and experiments regarding dimension, since it seems
fundamental to h-machine attributes.

Note some glimmer of ideas in this vein began in 2017; see "[See ref
to here from 8/5/2020]", above.

"Dimension" is a cute term, but really what's defined here is more
akin to "order" in set theory. I.e., given a family of subsets S over
a universe X, the order of S is the largest number of sets in S in
which a given x element-of X appears. So e.g. if X = {1,2,3,4,5} and S =
{{1,2},{1,3},{3,4},(1,4},{5}}, then the order is 3, since the element 1
appears in 3 subsets, the element 3 appears in 2 subsets, and all
other elements each appear in just 1 subset.

We can also define order as the largest number n of non-empty n-way
intersections among the elements of S.

In the case of the h-machine, the universe is G, the hypergraph of the
h-machine. Imagine a G that is completely "built", i.e., some set of
rules has run to completion. Thus the preds of the rules of G will all
match subgraphs of G. Each pred set thus induces a cover on a subset
of G, i.e., each pred maps to a set of subsets of the edges of G. 

Call one of these families P, and p1...pn elements, i.e. edge sets, of
P. The *dimension* of P is largest number of edges in common with
p1...pn, i.e., the order of P.

Operationally, this is viewed as the number of times an edge is needed
to match a rule successfully, where by successful we mean a rule which
ran and produced new edges. So if a rule ran 10 times producing new
edges, and some edge was needed 5 times, and no other edge was needed
more than 5 times, then the dimension of the rule is 5. If no edge is
needed more than once, then the rule has a dimension of one.









----------------------------------------------------------
8/20/20 -- Info on papers and history thereof

[8/20/20] Wolfram has written about hypergraphs and physics -- looks
like he lifted my ideas! Maybe not, but now I wonder if he or someone
on his staff read my paper.

Earliest form of complexity.doc looks like it's from 2008. 

Submitted to Wolfram's "Complex Systemss Journal" in 2013. Revceived email ack from them 
wrt the sdubmission:


	[WR #3230387] Lawrence Stabile: Chaos and Complexity
	Complex Systems Journal
	<info@complex-systems.com>
	12/18/2013 8:39 AM
	To  lstabile@alum.mit.edu  
	
	Dear Lawrence Stabile,
	
	Thank you for submitting the following paper to Complex Systems.
	
	Lawrence Stabile, "Chaos and Complexity"
	
	We will assign a reviewer and update you once comments have been 
	received. For future inquiries regarding the status please send email to 
	info@complex-systems.com  including the subject of this email:   [WR 
	#3230387] Lawrence Stabile: Chaos and Complexity
	
	Sincerely,
	
	Complex Systems
	PO Box 6149
	Champaign, IL 61826
	info@complex-systems.com

I had prodded them a couple of times over the next year and they said
at least once that they were still in process. Finally heard again in
Feb 2017 that I could revamp it to include more of NKS stuff and
resubmit. Final email from Todd Rowland, timestamped 2/3/2017 3:44 PM:

	Dear Larry Stabile,
	
	I have good news and bad news. Today is my last day as the managing editor 
	of Complex Systems (http://www.complex-systems.com/).
	
	This means that, if you are still interested, you can improve your paper 
	and resubmit it so that it will be considered by the new editor.
	
	I highly recommend adding more graphics, but especially explaining the 
	connections to Stephen Wolfram's "A New Kind of Science" 
	(http://www.wolframscience.com/nksonline/toc.html), which is the epitome 
	of Complex Systems publications.
	
	The main issue with your paper was that it did not appear to take into 
	account the ideas and evidence in that book.
	
	Thank you for submitting your work. I apologize again for the delays.
	
	Sincerely,
	
	Todd Rowland
	Managing Editor
	Complex Systems


I did not follow up, in particular since it didn't seem correct to
push it further toward NKS. NKS provided some ideas but not all, and
it would not have been correct to try to pump it up.  I instead began
to find other outlets. However *at least one person* read the paper at
Wolfram, at least I know that.

I worked more on the h-machine system and experiments and wrote the
h-machine paper. This had concrete ideas and results, as opposed to
"Chaos and Complexity", which had theory and philosophy.  The full
paper referred to the original "Chaos and Complexity", but I removed
those references and wrote a self-contained version for an IEEE conf,
from which it was rejected (Aug 2017). See above "Notes on submission
to IEEE ICRC 2017"

----------------------------------------------------------
9/22/20 -- Removed edge-sequence code

This includes the edge-entry struct: prev, next, and seqno. 

Old version saved in h.9.22.20.lisp.

I had various ideas for using the time sequence of edge creation in
optimization algorithms. I had started a version of execute-rule-dep
(x-execute-rule-dep), which would have attempted to prune down edges
incident on preds in the rule-dep graph by looking at whether they
were "new", and melding that with the dimension of the pred. The
dimension is only calc'ed empirically though, at this point -- i.e., I
have not found a way to derive dimension from the rules alone -- and
thus a feedback loop is required to use this info.

These ideas -- newness of edges, use of dimension, rule-dep graphs --
don't seem to be converging on good general rule execution methods. So
while most of the code will be left in, e.g., dimension calcs,
rule-dep exec, those which affect perf are being removed. In
particular this is the next/prev edge history structure.

I have concluded recently that the best optimization will come about
via improvemnets to the existing rule eval structure.  In essence, the
notion of local rules attached to nodes, and providing rules which
pass rules to other nodes, is a pretty good one, and we should keep
it. Also global rules work, though slow, thus providing a
development/optimization path.

Other than the problem of basic optimization of rules into such local
mechanisms, the only other issue wrt performance is the *expansion*
problem, i.e., keeping the search for matching subgraphs as local to
the attaching object as possible (and not growing effectively with the
size of the graph).

I have concluded that this problem is fundamental to the H-machine
structure, as opposed to something such that a clever internal
optimization in the kernel will suffice. 

----------------------------------------------------------
2/13/21 -- Graphviz goals and issues

I would like to submit a set of graphs (gv files) to Graphviz for
display in their gallery. I propose to base this on the fft
butterflies and rule 30. One thing I sought to resolve is to show
that the butterfly diagram I generate is isomorphic to the standard
form that appears in many books, e.g., Opp 1975. After much effort to
generate array indices which can be used to correlate the nodes and
edges, I then used a graphviz editing tool, the Qt Visual Graph Editor
(installed). This works much better than dotty, which is too buggy to
be usable.

The file fft-8-with-indices.gv (and subsequent fft-8-with-indices.svg)
show the original graphviz-produced layout. fft-8-with-indices.xgr
shows the hand-edited form thereof, where I moved around edges and
nodes until I got a diagram which shows the input array in normal
order and tne output array in bit-reversed order. This diagram is
identical to that in Opp 1975 page 300 fig. 6.12.The app used for this
is the "Qt Visual Graph Editor", C:\Program
Files\QVGE\bin\qvgeapp.exe.

Details of fft and rule 30 follow.


g10.gv -- 125-level rule-30. g10.svg took about 3 hours to produce:
"c:\Program Files (x86)\Graphviz2.38\bin\neato.exe" g10.gv -s -Tsvg | sed -e "s/<svg.*$/\<svg/" > g10.svg
g10.gv took awhile to produce. Higher than that and Clisp ran out of
space. This was 2/25/21 and I'm working on further efficiencies.

----------------------------------------------------------
9/20/22 Papers on nerdlynotions

Noticed that links in the full paper hmachine.docx (and posted
hmachine.pdf) were stale and needed updating.

The original paper and the target of the link is in
c:\Users\lstabile\h (~/h). The paper is ~/h/hmachine.docx. This should
remain the ground source file.  The link should be to ~/h/pub.

So ~/h should not be deleted nor its name changed.

On 9/20/22:

	Copied ~/h/pub to google drive/H-Machine. The link is:
		https://drive.google.com/drive/folders/1nbd3awkaKdWRUWhX1XA87uz6R5Qzkriu?usp=sharing

	Removed ieee version of h-machine from nerdlynotions. Doesn't
		seem to be any point in showing it there.

	Updated ~/h/hmachine.docx with new links and produced ~/h/hmachine.pdf therefrom.

	Copied ~/h/hmachine.pdf to ~/hmachine/doc, replacing the on there,
		for checking to github.

Should have the goal of referencing all this stuff to github at some
point -- when I get to updating the paper(s).

----------------------------------------------------------
11/29/22 -- Updates to rule pools and their denotation in rules

Tag "global-change"

Removed exposure at the H level of the global rule pool. However, the
global pool still exists, and is important esp. as a development
vehicle. The idea is that the global rule pool should normally be
small or empty, e.g., one might set a rule to global to find some bug
in propagation. And of course during initial development some set of
rules may be all global.

We retain the local rule pool, however, and its exposure, since it's a
convenient way to find rules that can then be passed around, modified,
etc. Note it's possible to do something similar entirely within H, but
it's a lot easier built-in, at least at this stage of H development.

To support this, the exposure of a rule is now specifiable in three
orthogonal ways:

   (rule
	(local)						Put on local rule pool (exposed in H as local-rule-pool-node)
	(global)					Put on global rule pool (not exposed in H)
	(attach-to x)				Attach to x
		...)

If a rule is devoid of any of these designators, then global is
assumed. Otherwise just the stated indicators take effecvt.

An empty nested rule is by default attached only at the point of
definition, e.g.,

	(add
		(?x rule (rule (...))))

attaches the nested rule onnly to ?x.

The rule can also be declared local or global or attached within its
definition.

As part of this, made other basic changes in h.lisp, flagged at the
time with the "global-change" marker:

- Calls into question some basics like addraw, add, and
  define-obj. The notion of an "object" may not need to be formalized
  in the kernel. Needs more work.

- Changed primary scan loop to do global too by default:
  execute-global-all-objs-loop (&key (queue-rule-mode :local-global) 
							         (scan-rule-mode :local-global #| :local-only |#))

- has-rules (node) used to check for attached rules and whether the node
  had a global-rule-pool pointer. Now just returns t.

- Fixed major issues with matching:
		- Ability to match clauses like (?x next ?x)
		- Matching when two vars bind to the same value
		- Not bailing in matching on root-vars, as one can match then fail later.
	These changes overall caused the "-sing" ("-singular") versions of
	fft-comb-rule-next and copy-array-struct-next to be unnecessary.

5/16/23 Status update: fixing code marked with the tag and removing
the tag, or moving the tag and comment here for later consideration.

	 - Remove add of lrp to global-node in objgraph/init, and put in
       an init rule instead?

	  (defm addraw (n a v)				;; global-change: Since this just calls add-edge perhaps change to add-triplet


	  (defm add (node-or-edge &optional a v)		;; global-change: This is used by some tests to inject edges and
													;; queue them. Perhaps define as an ACE adjunct.

----------------------------------------------------------
2/3/23 -- Revelations on rule determinism

Deterministic: D or det
Non-deterministic: ND or non-det

A deterministic rule is one which matches only one subgraph. This can
be so -- and normally is -- when attached to a given node, i.e., we
bind the root var and thus collapse the numnber of vars in the rule.

A non-deterministic rule matches many subgraphs, thus causing many
instantiations and possibly creating many new edges in one shot. But
such a rule may also then be redundant, and edges may keep matching or
being found during the match process, thus contributing to explosive
behavior.

A det rule may be optimized by various means:

	- When done, it may be detached from a node. A non-det rule can't,
	  since you don't know in general whether it will be needed
	  again. Note we *do* detach many non-det rules, e.g. in the rule
	  generators; this is ok since we know they won't be needed
	  again. But with a det rule we can potentially do it
	  automatically.

	- A subst match from the right root will find each matching edge
      in a context-free manner, using efficient (qet) lookup.

	- Intuitively, other optimizations seem possible, e.g.if we can
      crawl the rule edges detly and context-freely we may able to
      "compile" them down to something more primitive.

	- Non-det rules are still useful, esp. in building rules and other
      structures needed downstream. Let those rules "burn hot" to
      establish the more optimized structures, which will run "cool".

	- A rule *may* have same-property edges on a node and be det, but
	  if all props are unique then it *must* be det (assuming it's
	  var-connected). I.e., we can find a prop-unique path from some
	  root to all other nodes.

----------------------------------------------------------
4/16/23 -- Subst-match

Uses qet system to control the search for a match, thus greatly
reducing the computational complexity. Chains not needed anymore.

Settled on a basic algorithm that essentially walks one var at a
time. We make passes on a set of pred "rows", into which are
substituted values from matching edges filled in via qets. The basic
loop will cycle through all possible edge combinations, thus
producing all matches.

Key in selecting the next variable to bind and subst is selecting the
pred which has in a given pass the least number of edges retrieved by
get-edges-from-subqet. This results in huge search savings. For
instance we can go after a rule in a rule pool by a given name, and
this method will automatically select a binding via the name first,
thus whittling down typcially to one the number of further edges to be
considered to lookup the rule.

Also, we pass a pred-info data structure which records whether a node
in a pred is "data" or "var". A var can become data when it is the
value of a binding which comes infrom a match. E.g., a valid binding
is (?x ?y). The all-matches-aux algorithm handles this automatically,
but subst-match needs special handing.

*********
The loop consists of:

	Check that the set of preds is all-consts, and if so if all those
	edges exist. This then is a solution and the associated env is pushed
	on the solution list. 
	
	Else, if not all-consts, match on the set of preds. The match
	scans the preds and looks up qets by spltting out the vars and
	looking up each const sequence, if that sequence is of length 2 or
	more. First it just looks up the lengths (which can be optimized
	by caching or keeping running counts). It selects the pred with
	the min number of edges, and then matches the pred against these
	edges and emerges with a set of envs, each env consisting of one
	binding; all of these are for the same var.
	
	We then do a subst for each of these envs and recursively call the
	loop. The call passes env chains and at bottom, when
	all-consts/edges-exist, each chain is there ready to be stored in
	the solution table.
	
	When all the loops have ended, we convert the solution table to a list
	of envs and return that.

*********

This algorithm was develpoed with much effort. The qet system should
offer excellent indexing, with bounded access times. We'll have to see
where to take it next.

The one-var-at-a-time model could model possibly be made better, but
won't fool with it now.

The fact that all combos are found exhaustively means we need to
control the search. So we limit to length 2 or great qet size for
lookups, since size one is essentiall get-edges, which can really
explode. Similarly we restrict the model only to root-vars which are
actual vars, not nodes, so we alsways have an initial binding to
reduce explosion. Matches on a non-var root are done with the old
model.

Changes to the qet system induced by the new rule format have an
impact on subst-match. Before, we just removed vars from a pred, and
that formed the qet.  Now, since the qets purely represent subseqs, we
need to remove the vars and return multiple qets, the possibly
noncontiguous set of subseqs. Each of these is then looked up for
edges, and the result unioned. Thus finding the "minimal number of
edges" may not be easy, though we can do a sum as an approx which is
probably ok.

----------------------------------------------------------
4/16/23 -- New Rule Format

Tag new-rule-format

This was a great breakthrough. I was able to eliminate the entire
"_elem" encoding for rules, adopting instead a much simpler system
based on edges of different sizes and use of prefixes.

This also resulted in the qet system becoming purely based on
sequences and sub-sequences, which is n^2 storage, rather than 2^n as
in the set-based qet system. In this respect it resembles more an
*oriented* abstract simplicial complex. The storage therefore with
this model is more readily bounded, while retaining generality.

Another result -- and what started it all I think -- is the
realization that the NN new-node system had issues. This showed up in
the rule copying tests. The new rule format eliminates the need to
allocate all sorts of new nodes for rules and nested rules. So now all
new nodes are done with the "scoped" new-node pred. (Just) one is
added for each nested rule. In this manner rules with new-node aspects
can be copied. This failed with NN.

Essentially, the new rule encoding is simply a set of expanded edges
in a rule, e.g.:

(rule
 (name r1)
 (pred
  (?x a ?y)
  (?y b 57))
 (add
  (?y c 42)))

translates to the following edges, with a new node n1:

	(n1 name r1)
	(n1 pred ?x a ?y)
	(n1 pred ?y b 57)
	(n1 add ?y c 42)

So to decode the rule in the kernel, we simply take the clause
indicator and the rest of the list after that as its rule-edge.

The main nice thing here is that it nests. So a rule-generating rule
may simply say something like:

	(r add new-r pred ?x a ?y)

where r is the generating rule. So this add edge under r will add the
pred clause (?x a ?y) to new-r.

This nests by just adding more rule info on the left. The is similar
to how a protocol is "wrapped" in a new layer in networking, i.e., by
adding another "header" to the data packet.

But, to do copy-rule, we need to add a "rest var", since the encoding
of rules now uses variable-length edges. A rest-var is denoted ?*<x>
and when it appears in a pred means that it matches the remainder of
the list, assuming the vars and edges preceding it match. So it's
implicitly at the end of a pred and more than one rest var in a pred
is an error but operationally means nothing in the pred after that is
matched.

A rest var is the only type of var that may be bound to a
list. Multiple rest vars may appear in an add.  Their values are
spliced into the resulting edge.

With this, we can match a rule pred of arbitrary size, e.g.:

	(?r pred ?x next ?y)

This will match a rule with a pred specifying a next relation. For
instance we might get bindings:

	((?r n42)(?x ?a)(?y ?b))

For rules like rule copiers, we match on any pred a rule with:

	(?r pred ?*pred-rest)

Similarly for an add:

	(?r add ?*add-rest)

etc.

As an aside, we considered a "quoting" model, to specify in a pred
that we're searching for a literal var. But I deemed it not necessary,
and is certainly desirable to avoid it if possible. It appears that
being able to match against vars is sufficient to pick up whatever
patterns in rules we need.

----------------------------------------------------------
5/10/23 -- Eliminated hhash-tables

Clisp as it turns out has a way to define a test and hash fcn to pass
to a hash table. So not we're using CL hash tables, with specific
tests and hashes as needed. Look for uses of the macro
define-hash-table-test. See clisp doc, impnotes.html.

It's a marco, and function names are required, not functions
themselves. This makes it clumsier than it should be, but we live with
it.

The file hashtab.lisp was eliminated, and general CL hash table utils
moved to utils.lisp.

----------------------------------------------------------
5/16/23 -- Dumped log-value package

Not only did it use packages, which I don't like, but it wasn't really
all that useful. Having to grab a symvbol to get a value was too
tedious, and did not allow seeing issues at-a-glance very well. Better
is to hack with the *print dynamic vars.

----------------------------------------------------------
5/16/18 -- Supporting rule preds which are all vars (all-var-mod)

The tag all-var-mod is in the code and the needed changes are marked
and tests retained. But right now it's disabled.

The idea is that the subgraohs we need to match on are comcpletly
local, i.e., we have a small max degree of any node, sort of a
"manifold" constraint.  Then, markers for which we'd normally use
constant symbols, like attribute names, become local graph patterns,
Using all vars is a way to spec such a skeleton.

Things obviously get tougher computationally, since we now need a lot
more nodes, and ideally massive parallelism. A serial system would
need much better indexing than the subqet system.

But, this model is distinctly more "molecular" than one with
constants. Rule copying coupled with this makes a good purely local
model. This might be worth pursuing sometime.

----------------------------------------------------------
5/21/23 -- Qets, Sets, Seqs, and Strings, oh my!

It turns out there are conficting defintions of various orderings in
math and cs.

Set: a well-known standard definition, unorderd, unduplicated objects.

Sequence: Tuple.

Subsequence: Any subset of a sequence which retains the transitive
	sequence order (like the original subqets). So (1 2) ia a subseq
	of (a 1 b 2 c). CL and some other languages get this wrong, with
	subseq operations only applying to contiguous subseqs.

String: Tuples.

Substring: Contiguous subseq.

Hypergraph: Unordered edges.

Ordered hypergraph: Most modern deefinition seems to be the edge-bipartite form. 

Oriented hypergraph: Standard seems to be +-1 weighted edges in the
	bipartite rep of the hypergraph.  But people like Bill Kay say
	it's edges ordered as tuples (like lists). Not clear if/how dups
	are permitted in these cases.

Clearly sets are the most general, and holding all subsets provides
the classic abstract simplicial complex (ASC).

The next level of specificity is to consider the sets ordered, but
without necessarily allowing dups. We then take all subseqs (in the
math sense above, like the original subqets), which is still 2^n such
subseqs. But we don't realy have asn ASC. A further extension is to
allow dups.

Next we can go to strings, i.e., to form the ASC-like structure we use
substrings, contiguous subseqs. This is on the order n^2 so we save a
lot this way.  This is where the qet system is now and it appears to
work well. So a qet will be a "string".

Once we go beyond sets, set operations must give way to operations
more appropriate to srtings (qets). Operations like intersection and
union require more elaborate counterparts, and have limited
use. Similarly, tuple operations like concat, splicing, etc. are not
useful in the H context. The one property used is containment, i.e.,
is one string a substring of another? This is all we need.

---------------
Bill Kay
Ph.D. Mathematics. M.S. in Computer Science.
Ryerson University & The Fields Institute.
https://math.ryerson.ca/~bkay/index.html

https://math.ryerson.ca/~bkay/gist_propo.pdf,  "Hey Bill, what’s the
		deal with The minimum number of edges in uniform hypergraphs
		with Property O?":

			"...Given a graph G, an orientation of G is a graph
			obtained by taking each edge and putting an order on it
			(that is, turning the edges which are sets into
			2-tuples). Similarly, one can take a kuniform hypergraph
			(that is, and graph whose edges are k-sets) and orient it
			by turning the edges into k-tuples. Any graph obtained
			this way is an oriented k-graph. ..."
---------------
k-uniform HGs -- with ordered edges? Does not seem that way a
	"k-tuple" in this paper appears to be just a set of size k.
Called "independence sets" in the lit.
  ~/ErdosSetsOfIndependentEdgesOfAHypergraph1976-42.pdf
	~/Bollobas.pdf
---------------
~/ExtremalProblemsOrderedHypergraphs.pdf

https://par.nsf.gov/servlets/purl/10341330#:~:text=An%20ordered%20hypergraph%20is%20a,of%20problems%20in%20combinatorial%20geometry.

Extremal problems for convex geometric
hypergraphs and ordered hypergraphs
Zoltán Füredi, Tao Jiang, Alexandr Kostochka, Dhruv Mubayi, and
Jacques Verstraëte

Abstract. An ordered hypergraph is a hypergraph whose vertex set is
linearly ordered, and a convex geometric hypergraph is a hypergraph
whose vertex set is cyclically ordered. Extremal problems for ordered
and convex geometric graphs have a rich history with applications to a
variety of problems in combinatorial geometry. In this paper, we
consider analogous extremal problems for uniform hypergraphs, and
determine the order of magnitude of the extremal function for various
ordered and convex geometric paths and matchings. Our results
generalize earlier works of Braß–Károlyi– Valtr, Capoyleas–Pach, and
Aronov–Dujmovic–Morin–Ooms-da Silveira. We also provide a new ˇ
variation of the Erdos-Ko-Rado theorem in the ordered setting.
---------------

----------------------------------------------------------
6/16/23 -- Tensor representation of hypergraphs

Found some pretty recent papers on this:

~/OuvrardAdjacencyAndTensorRepresentationInGeneralHypergraphs.pdf

This is a good overview of the various reps, and in particular
develops the various forms of the adjacency tensor, i.e., Tijk = 1 iff
nodes ni, nj, and nk form an edge.

There are various ways to get it to behave properly for hypergraphs
not k-uniform, e.g. in Banerjee they define all perms using dups of
some of the elements. 

Banerjee2017SpectraOfGeneralHypergraphs.pdf

Ankit Sharma
https://scholar.google.com/citations?user=8CsyV4MAAAAJ&hl=en
Sharma2020JointSymmetricTensorDecompositionForHypergraphEmbeddings.pdf

This paper uses a 2D matrix of incidences of nodes and edges. But they
also define a node-node tensor.

Feng2019Hypergraph_Neural_Networks.pdf

----------------------------------------------------------
8/15/23 -- New "is" rule model

(rule
 (name is)
 (attach-to is)
 (pred
  (?x is ?y)
  (?y has ?r*))
 (add
  (print is)
  (?x ?*r))
 (del
  (?x is ?y)))

Then build an obj to inherit from e.g.:

(treeobj has rule n42)
(treeobj has rule n57)
(treeobj has type tree-node)

and assert (add) (<x> is treeobj)

A possible use here is encapsulating optimizations, e.g., rule order. 

We might for example have

(treeobj has rule n42)
(treeobj has rule n57)
(treeobj has rule-order n57 n42) 

So any x for which (x is treeobj) would have this rule order directive, to be used by execute-obj.

----------------------------------------------------------
8/19/23 -- New rule-order relation for execute-obj

(x rule-order r1 r2 ...)

An object may have many rules, so when executing the object, some
given order may be desirable. A particular use case is that a rule may
delete another, the former's success acting as an inhibitor of the
latter, i.e., it won't even get tested.

This is a form of if-then-else. For instance we can pass a rule that
operates on leaves down the tree, and each success of an
intermediate-node rule deletes the leaf-based rule, thus forestalling
testing.

When the deleting rule fails -- because the node is not intermediate
-- the persecuted rule gets a chance at last, and will succeed because
now it's being applied to a leaf.

----------------------------------------------------------
8/27/23 -- Experiment with making fft-comb's separate nodes

See new-fft.lisp, comment therein:

"Interesting experiment, where we replaced the use of the constant
fft-comb with a generated node, along with a name and label
of "fft-comb".  When left as such, we get 4-var pred edges, e.g.,
(?x0 ?x1 ?f ?y), with ($f name fft-comb). This works, but is very slow,
taxing subst-match heavily. We fixed it using rule gen, from fft-rule,
below.

"This looks promising and points out adjustments we may need to rule
scoping and such, e.g., there is no way to generate a rule and add it
as a rule prop to more than one node, within the framework of nested
rules. There seems to be a way to do this via flattening, and we
should examine that."

----------------------------------------------------------
8/31/23 -- Run dels even on a redundant match

A node may delete a rule after running it, but that rule may reappear
during rule propagation. If it matches and no new edges are produced,
run the dels anyway.  On the std bench this reduces redundancy and
boosts efficiency:

                    Old          New

||MAIN           15.38588      9.941568
||ME-EFFICIENCY   0.14948665   0.17226692 
||ME-REDUNDANCY   0.21765913   0.10269759 
||ME-FAILURE      0.6328542    0.7250355  

Failure rate is now higher! 

Also, we can now use this behavior to control more easily circular rule
propagation, where when we come around to the starting node no edges
are added, and thus the rule does not continue to propagate, and the
rule is deleted from the starting node, thus forestalling more
redunndant testing. See the new cas- rules in fft.lisp.

----------------------------------------------------------
2/13/24 -- Brief foray into quotient sets, pasting maps, genus, and Euler characteristic

An initial look into this area. Inspired by the idea that if you take
a recursive rule and "paste" an add edge back to a matching pred edge
(the recursive loop), then the set of rule edges should form an asc
with at least one hole (at least genus 1). The intuition here is that
a "flat" rule can be laid out in a manner that's "planar" wrt its
dimension d but non-planar when edges are pasted together, as in
making a cylinder or moebius strip. The pasting thus requires
dimension d+1 to be "planar". Thus a conjecture is that recursion
always increases the genus of an asc.

Experiments on forms of rules to look at Euler characteristic and
genus are in test.lisp. Look for "Brief foray into quotient sets..."

I tried using various tools to look at the resulting 2d triangulation
of these ASCs, but these don't seem to help much, and I need more
automation and better graphics tools. Used Gephi to look at some
constructions, but it's pretty awful. Dotty was bad too, but now
there is not much for gv out there.

The formulas for Euler characteristic (chi) don't seem to be
consistent in the literature, i.e., I keep getting off-by-one (or two)
sorts of deltas wrt expectations for chi and the genus (g). The
formula relating g and chi differs if the surface is orientable vs
non-orientable, and determining that does not look easy.

Oliver Knill, The energy of a simplicial complex, ~/KnillEnergySimplicialComplex.pdf.
BUT desired formulas only appear in this summary, the full paper for which I cannot access:
	https://www.sciencedirect.com/science/article/abs/pii/S0024379520302019

Bjarke Hammersholt Roune and Eduardo Saenz-De-Cabez´On, Complexity And
Algorithms For Euler Characteristic Of Simplicial Complexes,
~/RouneAlgsForEulerCharOfSimplicialComplexes.pdf.

Notes from a Stanford course, looks worth pursuing:
	~/SimplicialComplexesInComputationalTopology.pdf
	
The Implementation of the Colored Abstract Simplicial Complex and Its
Application to Mesh Generation,  Christopher T. Lee, John B. Moody,
Rommie E. Amaro, J. Andrew Mccammon, And Michael J. Holst, University
of California San Diego, CA,
https://dl.acm.org/doi/fullHtml/10.1145/3321515.

Description of chi, table and pics of polyhedra, mobius strips, ASCs, etc:
	https://en.wikipedia.org/wiki/Euler_characteristic

Closed vs open surfaces (related to genus formula):
	https://en.wikipedia.org/wiki/Surface_(topology)#Closed_surfaces

----------------------------------------------------------
7/17/24 -- Major change in keeping stats and so forth

Archived in archive/h.7.17.24.lisp. Also archived test.lisp Removed
from h.lisp succ-rule-freq, rule-freq-graph, failure-handling
experiment, successful-exec-obj-list, freq-obj-queue, failed-queue.

Generally, I need a new approach to the optimization algorithm, and
I'm cleaning out some of the stats to try to clear the thought
process.

----------------------------------------------------------
7/29/24 -- Diagnosing rules not triggering at large sizes

Tried fft with the usual local rules and the butterfly diagram was
incomplete for n=6 and above. Having the failure occur only at such
large sizes was a challenge to diagnose. The culprit was traced to
cas-zero, but only after generating a set of diagrams of different
types. First, I went to the "pure fft" model for a little bit of
simplification. Then I generated a set with butterflies etc., and a
separate diagram of just is-elem-of, next, zero, and others as
needed. This allowed me to see a missing zero. I tried most of the
usual changes with root vars and so forth, but it did not fix it.  But
a good technique was to define a new rule, cas-zero-x, which was
purely global. This fixed it, thus giving me a good direction to
head. The problem turned out to be a delete in cas-rule-mod.



(rule
 (name cas-zero-x)
 (disabled)
 (global)
 (pred
  (?a copy-array-struct ?a1)
  (?e0 is-elem-of ?a)
  (?ae0 is-elem-of ?a1)
  (?ae0 ref ?e0)
  (?e0 zero))
 (add
  (print cas-zero-x ?this-obj ?root-var ?a ?a1 ?e0 ?ae0)
  (?ae0 zero)
  (?ae0 cas-zero)
  (?a1 casz-ref1 ?ae0)
  (?a casz-ref ?e0)
  ))
